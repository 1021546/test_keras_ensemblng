Using TensorFlow backend.
2017-12-18 00:30:40.913149: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-18 00:30:40.913325: W C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU
computations.
x_train shape: (50000, 32, 32, 3) | y_train shape: (50000, 10)
x_test shape : (10000, 32, 32, 3) | y_test shape : (10000, 10)
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 32, 32, 3)         0
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 32, 32, 96)        2688
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 32, 32, 96)        83040
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 32, 32, 96)        83040
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 15, 15, 96)        0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 15, 15, 192)       166080
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 15, 15, 192)       331968
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 15, 15, 192)       331968
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 7, 7, 192)         0
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 7, 7, 192)         331968
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 7, 7, 192)         37056
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 7, 7, 10)          1930
_________________________________________________________________
global_average_pooling2d_1 ( (None, 10)                0
_________________________________________________________________
activation_1 (Activation)    (None, 10)                0
=================================================================
Total params: 1,369,738
Trainable params: 1,369,738
Non-trainable params: 0
_________________________________________________________________
Train on 40000 samples, validate on 10000 samples
Epoch 1/20
40000/40000 [==============================] - 1251s - loss: 2.3026 - acc: 0.1006 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 2/20
40000/40000 [==============================] - 1249s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 3/20
40000/40000 [==============================] - 1266s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 4/20
40000/40000 [==============================] - 1279s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 5/20
40000/40000 [==============================] - 1254s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 6/20
40000/40000 [==============================] - 1241s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 7/20
40000/40000 [==============================] - 1241s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 8/20
40000/40000 [==============================] - 1241s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 9/20
40000/40000 [==============================] - 1241s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 10/20
40000/40000 [==============================] - 1241s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 11/20
40000/40000 [==============================] - 1242s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 12/20
40000/40000 [==============================] - 1247s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 13/20
40000/40000 [==============================] - 1241s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 14/20
40000/40000 [==============================] - 1242s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 15/20
40000/40000 [==============================] - 1240s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 16/20
40000/40000 [==============================] - 1240s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 17/20
40000/40000 [==============================] - 1242s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 18/20
40000/40000 [==============================] - 1242s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 19/20
40000/40000 [==============================] - 1242s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 20/20
40000/40000 [==============================] - 1241s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
0.0001
1.0
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 32, 32, 3)         0
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 32, 32, 96)        2688
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 32, 32, 96)        83040
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 16, 16, 96)        83040
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 16, 16, 192)       166080
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 16, 16, 192)       331968
_________________________________________________________________
conv2d_15 (Conv2D)           (None, 8, 8, 192)         331968
_________________________________________________________________
conv2d_16 (Conv2D)           (None, 8, 8, 192)         331968
_________________________________________________________________
conv2d_17 (Conv2D)           (None, 8, 8, 192)         37056
_________________________________________________________________
conv2d_18 (Conv2D)           (None, 8, 8, 10)          1930
_________________________________________________________________
global_average_pooling2d_2 ( (None, 10)                0
_________________________________________________________________
activation_2 (Activation)    (None, 10)                0
=================================================================
Total params: 1,369,738
Trainable params: 1,369,738
Non-trainable params: 0
_________________________________________________________________
Train on 40000 samples, validate on 10000 samples
Epoch 1/20
40000/40000 [==============================] - 944s - loss: 2.3040 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 2/20
40000/40000 [==============================] - 939s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 3/20
40000/40000 [==============================] - 936s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 4/20
40000/40000 [==============================] - 938s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 5/20
40000/40000 [==============================] - 941s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 6/20
40000/40000 [==============================] - 943s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 7/20
40000/40000 [==============================] - 929s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 8/20
40000/40000 [==============================] - 932s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 9/20
40000/40000 [==============================] - 938s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 10/20
40000/40000 [==============================] - 933s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 11/20
40000/40000 [==============================] - 929s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 12/20
40000/40000 [==============================] - 938s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 13/20
40000/40000 [==============================] - 933s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 14/20
40000/40000 [==============================] - 930s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 15/20
40000/40000 [==============================] - 932s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 16/20
40000/40000 [==============================] - 934s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 17/20
40000/40000 [==============================] - 932s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 18/20
40000/40000 [==============================] - 950s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 19/20
40000/40000 [==============================] - 953s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
Epoch 20/20
40000/40000 [==============================] - 934s - loss: 2.3026 - acc: 0.0997 - val_loss: 2.3026 - val_acc: 0.1014
0.0001
1.0
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 32, 32, 3)         0
_________________________________________________________________
conv2d_19 (Conv2D)           (None, 28, 28, 32)        2432
_________________________________________________________________
conv2d_20 (Conv2D)           (None, 28, 28, 32)        1056
_________________________________________________________________
conv2d_21 (Conv2D)           (None, 28, 28, 32)        1056
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 14, 14, 32)        0
_________________________________________________________________
dropout_1 (Dropout)          (None, 14, 14, 32)        0
_________________________________________________________________
conv2d_22 (Conv2D)           (None, 12, 12, 64)        18496
_________________________________________________________________
conv2d_23 (Conv2D)           (None, 12, 12, 64)        4160
_________________________________________________________________
conv2d_24 (Conv2D)           (None, 12, 12, 64)        4160
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 6, 6, 64)          0
_________________________________________________________________
dropout_2 (Dropout)          (None, 6, 6, 64)          0
_________________________________________________________________
conv2d_25 (Conv2D)           (None, 4, 4, 128)         73856
_________________________________________________________________
conv2d_26 (Conv2D)           (None, 4, 4, 32)          4128
_________________________________________________________________
conv2d_27 (Conv2D)           (None, 4, 4, 10)          330
_________________________________________________________________
global_average_pooling2d_3 ( (None, 10)                0
_________________________________________________________________
activation_3 (Activation)    (None, 10)                0
=================================================================
Total params: 109,674
Trainable params: 109,674
Non-trainable params: 0
_________________________________________________________________
Train on 40000 samples, validate on 10000 samples
Epoch 1/20
40000/40000 [==============================] - 74s - loss: 2.1146 - acc: 0.2265 - val_loss: 1.7438 - val_acc: 0.3420
Epoch 2/20
40000/40000 [==============================] - 72s - loss: 1.6615 - acc: 0.3856 - val_loss: 1.5234 - val_acc: 0.4437
Epoch 3/20
40000/40000 [==============================] - 72s - loss: 1.5422 - acc: 0.4360 - val_loss: 1.4650 - val_acc: 0.4571
Epoch 4/20
40000/40000 [==============================] - 72s - loss: 1.4628 - acc: 0.4654 - val_loss: 1.3744 - val_acc: 0.5008
Epoch 5/20
40000/40000 [==============================] - 72s - loss: 1.4047 - acc: 0.4867 - val_loss: 1.3145 - val_acc: 0.5267
Epoch 6/20
40000/40000 [==============================] - 72s - loss: 1.3520 - acc: 0.5060 - val_loss: 1.2550 - val_acc: 0.5520
Epoch 7/20
40000/40000 [==============================] - 72s - loss: 1.3119 - acc: 0.5227 - val_loss: 1.2794 - val_acc: 0.5348
Epoch 8/20
40000/40000 [==============================] - 72s - loss: 1.2649 - acc: 0.5454 - val_loss: 1.2015 - val_acc: 0.5747
Epoch 9/20
40000/40000 [==============================] - 72s - loss: 1.2312 - acc: 0.5592 - val_loss: 1.2548 - val_acc: 0.5520
Epoch 10/20
40000/40000 [==============================] - 74s - loss: 1.2053 - acc: 0.5692 - val_loss: 1.1218 - val_acc: 0.6020
Epoch 11/20
40000/40000 [==============================] - 73s - loss: 1.1766 - acc: 0.5768 - val_loss: 1.1266 - val_acc: 0.6011
Epoch 12/20
40000/40000 [==============================] - 73s - loss: 1.1510 - acc: 0.5864 - val_loss: 1.0816 - val_acc: 0.6138
Epoch 13/20
40000/40000 [==============================] - 72s - loss: 1.1310 - acc: 0.5947 - val_loss: 1.0859 - val_acc: 0.6206
Epoch 14/20
40000/40000 [==============================] - 72s - loss: 1.1135 - acc: 0.6036 - val_loss: 1.1032 - val_acc: 0.6081
Epoch 15/20
40000/40000 [==============================] - 72s - loss: 1.0985 - acc: 0.6088 - val_loss: 1.0112 - val_acc: 0.6413
Epoch 16/20
40000/40000 [==============================] - 72s - loss: 1.0804 - acc: 0.6169 - val_loss: 1.0127 - val_acc: 0.6445
Epoch 17/20
40000/40000 [==============================] - 73s - loss: 1.0650 - acc: 0.6241 - val_loss: 1.0185 - val_acc: 0.6371
Epoch 18/20
40000/40000 [==============================] - 73s - loss: 1.0462 - acc: 0.6315 - val_loss: 1.0420 - val_acc: 0.6300
Epoch 19/20
40000/40000 [==============================] - 73s - loss: 1.0352 - acc: 0.6333 - val_loss: 0.9928 - val_acc: 0.6533
Epoch 20/20
40000/40000 [==============================] - 72s - loss: 1.0231 - acc: 0.6381 - val_loss: 1.0156 - val_acc: 0.6435
0.0001
9.1362
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to
====================================================================================================
input_1 (InputLayer)             (None, 32, 32, 3)     0
____________________________________________________________________________________________________
conv2d_19 (Conv2D)               (None, 28, 28, 32)    2432        input_1[0][0]
____________________________________________________________________________________________________
conv2d_20 (Conv2D)               (None, 28, 28, 32)    1056        conv2d_19[0][0]
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 32, 32, 96)    2688        input_1[0][0]
____________________________________________________________________________________________________
conv2d_21 (Conv2D)               (None, 28, 28, 32)    1056        conv2d_20[0][0]
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 32, 32, 96)    83040       conv2d_1[0][0]
____________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)   (None, 14, 14, 32)    0           conv2d_21[0][0]
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 32, 32, 96)    83040       conv2d_2[0][0]
____________________________________________________________________________________________________
conv2d_10 (Conv2D)               (None, 32, 32, 96)    2688        input_1[0][0]
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 14, 14, 32)    0           max_pooling2d_3[0][0]
____________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)   (None, 15, 15, 96)    0           conv2d_3[0][0]
____________________________________________________________________________________________________
conv2d_11 (Conv2D)               (None, 32, 32, 96)    83040       conv2d_10[0][0]
____________________________________________________________________________________________________
conv2d_22 (Conv2D)               (None, 12, 12, 64)    18496       dropout_1[0][0]
____________________________________________________________________________________________________
conv2d_4 (Conv2D)                (None, 15, 15, 192)   166080      max_pooling2d_1[0][0]
____________________________________________________________________________________________________
conv2d_12 (Conv2D)               (None, 16, 16, 96)    83040       conv2d_11[0][0]
____________________________________________________________________________________________________
conv2d_23 (Conv2D)               (None, 12, 12, 64)    4160        conv2d_22[0][0]
____________________________________________________________________________________________________
conv2d_5 (Conv2D)                (None, 15, 15, 192)   331968      conv2d_4[0][0]
____________________________________________________________________________________________________
conv2d_13 (Conv2D)               (None, 16, 16, 192)   166080      conv2d_12[0][0]
____________________________________________________________________________________________________
conv2d_24 (Conv2D)               (None, 12, 12, 64)    4160        conv2d_23[0][0]
____________________________________________________________________________________________________
conv2d_6 (Conv2D)                (None, 15, 15, 192)   331968      conv2d_5[0][0]
____________________________________________________________________________________________________
conv2d_14 (Conv2D)               (None, 16, 16, 192)   331968      conv2d_13[0][0]
____________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)   (None, 6, 6, 64)      0           conv2d_24[0][0]
____________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)   (None, 7, 7, 192)     0           conv2d_6[0][0]
____________________________________________________________________________________________________
conv2d_15 (Conv2D)               (None, 8, 8, 192)     331968      conv2d_14[0][0]
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 6, 6, 64)      0           max_pooling2d_4[0][0]
____________________________________________________________________________________________________
conv2d_7 (Conv2D)                (None, 7, 7, 192)     331968      max_pooling2d_2[0][0]
____________________________________________________________________________________________________
conv2d_16 (Conv2D)               (None, 8, 8, 192)     331968      conv2d_15[0][0]
____________________________________________________________________________________________________
conv2d_25 (Conv2D)               (None, 4, 4, 128)     73856       dropout_2[0][0]
____________________________________________________________________________________________________
conv2d_8 (Conv2D)                (None, 7, 7, 192)     37056       conv2d_7[0][0]
____________________________________________________________________________________________________
conv2d_17 (Conv2D)               (None, 8, 8, 192)     37056       conv2d_16[0][0]
____________________________________________________________________________________________________
conv2d_26 (Conv2D)               (None, 4, 4, 32)      4128        conv2d_25[0][0]
____________________________________________________________________________________________________
conv2d_9 (Conv2D)                (None, 7, 7, 10)      1930        conv2d_8[0][0]
____________________________________________________________________________________________________
conv2d_18 (Conv2D)               (None, 8, 8, 10)      1930        conv2d_17[0][0]
____________________________________________________________________________________________________
conv2d_27 (Conv2D)               (None, 4, 4, 10)      330         conv2d_26[0][0]
____________________________________________________________________________________________________
global_average_pooling2d_1 (Glob (None, 10)            0           conv2d_9[0][0]
____________________________________________________________________________________________________
global_average_pooling2d_2 (Glob (None, 10)            0           conv2d_18[0][0]
____________________________________________________________________________________________________
global_average_pooling2d_3 (Glob (None, 10)            0           conv2d_27[0][0]
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 10)            0           global_average_pooling2d_1[0][0]
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 10)            0           global_average_pooling2d_2[0][0]
____________________________________________________________________________________________________
activation_3 (Activation)        (None, 10)            0           global_average_pooling2d_3[0][0]
____________________________________________________________________________________________________
average_1 (Average)              (None, 10)            0           activation_1[0][0]
                                                                   activation_2[0][0]
                                                                   activation_3[0][0]
====================================================================================================
Total params: 2,849,150
Trainable params: 2,849,150
Non-trainable params: 0
____________________________________________________________________________________________________
Train on 40000 samples, validate on 10000 samples
Epoch 1/20
40000/40000 [==============================] - 2204s - loss: 1.5085 - acc: 0.6313 - val_loss: 1.4758 - val_acc: 0.6543
Epoch 2/20
40000/40000 [==============================] - 2189s - loss: 1.5019 - acc: 0.6352 - val_loss: 1.4401 - val_acc: 0.6739
Epoch 3/20
40000/40000 [==============================] - 2187s - loss: 1.4995 - acc: 0.6377 - val_loss: 1.4372 - val_acc: 0.6756
Epoch 4/20
40000/40000 [==============================] - 2190s - loss: 1.4942 - acc: 0.6395 - val_loss: 1.4636 - val_acc: 0.6553
Epoch 5/20
40000/40000 [==============================] - 2189s - loss: 1.4905 - acc: 0.6444 - val_loss: 1.4432 - val_acc: 0.6697
Epoch 6/20
40000/40000 [==============================] - 2192s - loss: 1.4861 - acc: 0.6471 - val_loss: 1.4907 - val_acc: 0.6425
Epoch 7/20
40000/40000 [==============================] - 2231s - loss: 1.4798 - acc: 0.6500 - val_loss: 1.4634 - val_acc: 0.6629
Epoch 8/20
40000/40000 [==============================] - 2194s - loss: 1.4768 - acc: 0.6537 - val_loss: 1.4310 - val_acc: 0.6816
Epoch 9/20
40000/40000 [==============================] - 2256s - loss: 1.4746 - acc: 0.6550 - val_loss: 1.4386 - val_acc: 0.6778
Epoch 10/20
40000/40000 [==============================] - 2242s - loss: 1.4682 - acc: 0.6585 - val_loss: 1.4445 - val_acc: 0.6702
Epoch 11/20
40000/40000 [==============================] - 2199s - loss: 1.4663 - acc: 0.6602 - val_loss: 1.4562 - val_acc: 0.6666
Epoch 12/20
40000/40000 [==============================] - 2170s - loss: 1.4618 - acc: 0.6600 - val_loss: 1.4171 - val_acc: 0.6911
Epoch 13/20
40000/40000 [==============================] - 2169s - loss: 1.4591 - acc: 0.6636 - val_loss: 1.4288 - val_acc: 0.6815
Epoch 14/20
40000/40000 [==============================] - 2169s - loss: 1.4545 - acc: 0.6676 - val_loss: 1.4161 - val_acc: 0.6932
Epoch 15/20
40000/40000 [==============================] - 2168s - loss: 1.4569 - acc: 0.6651 - val_loss: 1.4689 - val_acc: 0.6586
Epoch 16/20
40000/40000 [==============================] - 2169s - loss: 1.4527 - acc: 0.6685 - val_loss: 1.3959 - val_acc: 0.7017
Epoch 17/20
40000/40000 [==============================] - 2170s - loss: 1.4499 - acc: 0.6704 - val_loss: 1.4037 - val_acc: 0.6969
Epoch 18/20
40000/40000 [==============================] - 2169s - loss: 1.4448 - acc: 0.6734 - val_loss: 1.4263 - val_acc: 0.6840
Epoch 19/20
40000/40000 [==============================] - 2168s - loss: 1.4479 - acc: 0.6719 - val_loss: 1.4122 - val_acc: 0.6939
Epoch 20/20
40000/40000 [==============================] - 2171s - loss: 1.4440 - acc: 0.6745 - val_loss: 1.4136 - val_acc: 0.6928
0.0001
8.8838